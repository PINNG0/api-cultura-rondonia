"""
Runner principal do scraper da Funcultural.

Respons√°vel por:
- Carregar p√°ginas da listagem de not√≠cias
- Extrair blocos de eventos
- Processar cada evento individualmente
- Coletar conte√∫do detalhado da p√°gina interna
- Normalizar dados e retornar a lista final de eventos
"""

import logging
from time import sleep
from urllib.parse import urljoin

from scraping.config import URL_NOTICIAS
from scraping.fetch import get_soup
from scraping.processor import classify_blocks, preproc_content
from scraping.parser import norm_text


# ---------------------------------------------------------
# Coleta o conte√∫do detalhado da p√°gina interna do evento
# ---------------------------------------------------------
def scrape_details(url):
    """
    Acessa a p√°gina interna do evento e extrai:
    - texto detalhado
    - imagens internas
    - blocos de conte√∫do normalizados
    """
    logging.debug("üîç Coletando detalhes do evento: %s", url)

    soup = get_soup(url)
    if not soup:
        logging.warning("‚ö†Ô∏è Falha ao carregar p√°gina interna: %s", url)
        return []

    article = soup.find('article', class_='noticia-conteudo')
    if not article:
        logging.warning("‚ö†Ô∏è Estrutura inesperada: artigo n√£o encontrado em %s", url)
        return []

    # Pr√©-processa imagens internas
    imgs = preproc_content(article)

    # Classifica e organiza blocos de conte√∫do
    return classify_blocks(article, imgs)


# ---------------------------------------------------------
# Processa um √∫nico bloco da listagem (um card de evento)
# ---------------------------------------------------------
def process_single_block(bloco):
    """
    Extrai informa√ß√µes b√°sicas do card:
    - t√≠tulo
    - tag/categoria
    - imagem
    - link para p√°gina interna
    - data exibida
    E coleta o conte√∫do detalhado da p√°gina interna.
    """
    img_tag = bloco.find('img')
    banner_rel = img_tag['src'] if img_tag and img_tag.get('src') else ""

    title_tag = bloco.find('div', class_='titulo-noticia-pesquisa')
    title = title_tag.get_text(strip=True) if title_tag else "T√≠tulo n√£o encontrado"

    tag_tag = bloco.find('div', class_='tag-noticia')
    tag_evento = tag_tag.get_text(strip=True) if tag_tag else "Sem tag"

    link_tag = bloco.find('a')
    link_rel = link_tag['href'] if link_tag and link_tag.get('href') else None
    link = urljoin(URL_NOTICIAS, link_rel.strip()) if link_rel else None

    if not link:
        logging.warning("‚ö†Ô∏è Card ignorado: link inv√°lido.")
        return None

    date_tag = bloco.find('div', class_='datanot')
    data_exibicao = date_tag.get_text(strip=True) if date_tag else "Sem data"

    # Coleta conte√∫do detalhado
    blocks = scrape_details(link)
    if not blocks:
        logging.warning("‚ö†Ô∏è Conte√∫do detalhado vazio. Ignorando evento: %s", link)
        return None

    return {
        "titulo": norm_text(title),
        "tag_evento": norm_text(tag_evento),
        "blocos_conteudo": blocks,
        "imagem_url": banner_rel,
        "link_evento": link,
        "fonte": "Funcultural",
        "data_exibicao": data_exibicao
    }


# ---------------------------------------------------------
# Carrega uma p√°gina da listagem
# ---------------------------------------------------------
def load_page(pagina):
    """
    Carrega uma p√°gina da listagem de not√≠cias.
    """
    url = f"{URL_NOTICIAS}?page={pagina}"
    logging.info("üìÑ Carregando p√°gina %s", url)
    return get_soup(url)


# ---------------------------------------------------------
# Extrai os blocos de eventos da p√°gina
# ---------------------------------------------------------
def extract_results(soup):
    """
    Retorna todos os cards de eventos encontrados na p√°gina.
    """
    return soup.find_all('div', class_='resultado-pesquisa')


# ---------------------------------------------------------
# Verifica se existe pr√≥xima p√°gina
# ---------------------------------------------------------
def get_next_page(soup, pagina):
    """
    Verifica se existe link para a pr√≥xima p√°gina.
    """
    return soup.select_one(f'ul.pagination a[href*="page={pagina + 1}"]')


# ---------------------------------------------------------
# Runner principal ‚Äî coleta todos os eventos
# ---------------------------------------------------------
def scrape_all():
    """
    Percorre todas as p√°ginas da listagem e retorna
    uma lista completa de eventos normalizados.
    """
    logging.info("üöÄ Iniciando coleta de eventos da Funcultural...")

    all_events = []
    pagina = 1

    while True:
        soup = load_page(pagina)
        if not soup:
            logging.warning("‚ö†Ô∏è Falha ao carregar p√°gina %d. Encerrando.", pagina)
            break

        results = extract_results(soup)
        if not results:
            logging.info("‚úÖ Nenhum resultado encontrado na p√°gina %d. Encerrando.", pagina)
            break

        # Processa cada card da p√°gina
        for bloco in results:
            ev = process_single_block(bloco)
            if ev:
                all_events.append(ev)

        # Verifica se existe pr√≥xima p√°gina
        if not get_next_page(soup, pagina):
            logging.info("üìå √öltima p√°gina alcan√ßada (%d).", pagina)
            break

        pagina += 1
        sleep(0.1)  # evita sobrecarregar o servidor

    logging.info("‚úÖ Coleta conclu√≠da. Total de eventos coletados: %d", len(all_events))
    return all_events
